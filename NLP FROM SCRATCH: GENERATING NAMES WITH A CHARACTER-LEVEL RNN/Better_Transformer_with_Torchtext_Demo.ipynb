{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxEP3lOmiFsd"
   },
   "source": [
    "#Load torchtext and initialize XLM-R model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6P5DEMVJSHrD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/text/xlmr.large.encoder.pt\" to /home/zlabs-hwa-01/.cache/torch/hub/checkpoints/xlmr.large.encoder.pt\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.08G/2.08G [01:40<00:00, 22.3MB/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5.07M/5.07M [00:00<00:00, 23.5MB/s]\n",
      "Downloading: \"https://download.pytorch.org/models/text/xlmr.vocab.pt\" to /home/zlabs-hwa-01/.cache/torch/hub/checkpoints/xlmr.vocab.pt\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.85M/4.85M [00:00<00:00, 9.93MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "\n",
    "from torchtext.models import RobertaClassificationHead\n",
    "from torchtext.functional import to_tensor\n",
    "\n",
    "xlmr_large = torchtext.models.XLMR_LARGE_ENCODER\n",
    "classifier_head = torchtext.models.RobertaClassificationHead(num_classes=2, input_dim = 1024)\n",
    "model = xlmr_large.get_model(head=classifier_head)\n",
    "\n",
    "# Put model into inference mode (reduces runtime even without BT - esp for GPU execution, required for Better Transformer)\n",
    "model.eval()\n",
    "\n",
    "# Define input transform\n",
    "transform = xlmr_large.transform()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_i8J9wc_kDaL"
   },
   "source": [
    "# System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mJeGWu7kKCf",
    "outputId": "9640fe9f-4d53-437e-da9f-c84c6217afc3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zlabs-hwa-01/anaconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m cpu \u001b[38;5;241m=\u001b[39m platform\u001b[38;5;241m.\u001b[39mprocessor()\n\u001b[0;32m----> 6\u001b[0m gpu \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch cuda available: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:365\u001b[0m, in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the name of a device.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:395\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    246\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "cpu = platform.processor()\n",
    "gpu = torch.cuda.get_device_name(DEVICE)\n",
    "\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torch cuda available: {torch.cuda.is_available()}\")\n",
    "print(f\"CPU type: {cpu}\")\n",
    "print(f\"GPU type: {gpu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY-1QDY2nS52"
   },
   "source": [
    "# Check default sparsity support setting\n",
    "Sparsity support enables transformers to skip padding in inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mq61w_dQndlC",
    "outputId": "4a0696ff-e422-4465-d627-6d543ef82a53"
   },
   "outputs": [],
   "source": [
    "model.encoder.transformer.layers.enable_nested_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUokmFVhnvfK"
   },
   "source": [
    "# Benchmark setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqFcmAcaiDgE"
   },
   "source": [
    "###Define inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2ETItuybcwR"
   },
   "outputs": [],
   "source": [
    "small_input_batch = [\n",
    "               \"Hello world\",\n",
    "               \"How are you!\"\n",
    "]\n",
    "big_input_batch = [\n",
    "               \"Hello world\",\n",
    "               \"How are you!\",\n",
    "               \"\"\"`Well, Prince, so Genoa and Lucca are now just family estates of the\n",
    "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
    "if you still try to defend the infamies and horrors perpetrated by\n",
    "that Antichrist- I really believe he is Antichrist- I will have\n",
    "nothing more to do with you and you are no longer my friend, no longer\n",
    "my 'faithful slave,' as you call yourself! But how do you do? I see\n",
    "I have frightened you- sit down and tell me all the news.`\n",
    "\n",
    "It was in July, 1805, and the speaker was the well-known Anna\n",
    "Pavlovna Scherer, maid of honor and favorite of the Empress Marya\n",
    "Fedorovna. With these words she greeted Prince Vasili Kuragin, a man\n",
    "of high rank and importance, who was the first to arrive at her\n",
    "reception. Anna Pavlovna had had a cough for some days. She was, as\n",
    "she said, suffering from la grippe; grippe being then a new word in\n",
    "St. Petersburg, used only by the elite.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rPLNTKXiAZ4"
   },
   "source": [
    "###Select small or big input set\n",
    "\n",
    "Modify the assignment to input_batch below to select either the small_input_batch or big_inoput_batch, or substitute your own inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-21X4muzWbuX",
    "outputId": "6649b871-46a9-440e-8e69-ba08a6ad85b0"
   },
   "outputs": [],
   "source": [
    "input_batch=big_input_batch\n",
    "\n",
    "model_input = to_tensor(transform(input_batch), padding_value=1)\n",
    "output = model(model_input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXSw69v4hxe2"
   },
   "source": [
    "###Iteration count for performance measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HF3OFvIdbYnM"
   },
   "outputs": [],
   "source": [
    "ITERATIONS=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAHcLbYihta8"
   },
   "source": [
    "#Measure CPU  performance with slow and fast path, without and with sparsity\n",
    "\n",
    "Sparsity support enables transformers to skip padding in inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_td53nHgdQ0"
   },
   "source": [
    "### CPU performance without BT sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aItZ7EeghTT"
   },
   "outputs": [],
   "source": [
    "model.encoder.transformer.layers.enable_nested_tensor = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hLZsAR15Wkgv",
    "outputId": "cdc77881-f492-4a66-f1fa-2455efc9ebff"
   },
   "outputs": [],
   "source": [
    "print(\"slow path:\")\n",
    "print(\"==========\")\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
    "  for i in range(ITERATIONS):\n",
    "    output = model(model_input)\n",
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
    "\n",
    "print(\"fast path:\")\n",
    "print(\"==========\")\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
    "  with torch.no_grad():\n",
    "    for i in range(ITERATIONS):\n",
    "      output = model(model_input)\n",
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpIGGS5egqRh"
   },
   "source": [
    "###CPU performance with BT sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdPM9U1Qg5y0"
   },
   "outputs": [],
   "source": [
    "model.encoder.transformer.layers.enable_nested_tensor = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wq4_0E4-gzwV",
    "outputId": "97b086ba-eab5-4c41-d80c-c81928936dc7"
   },
   "outputs": [],
   "source": [
    "print(\"slow path:\")\n",
    "print(\"==========\")\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
    "  for i in range(ITERATIONS):\n",
    "    output = model(model_input)\n",
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
    "\n",
    "print(\"fast path:\")\n",
    "print(\"==========\")\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
    "  with torch.no_grad():\n",
    "    for i in range(ITERATIONS):\n",
    "      output = model(model_input)\n",
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBCDGjd-g2ny"
   },
   "source": [
    "#Measure DEVICE performance with slow and fast path, without and with sparsity\n",
    "\n",
    "Please ensure that the runtime has GPUs enabled to see the performance benefits of Better Transformer fastpath execution on GPUs. You can confirm and change the Runtime type in the Google Colab menu with (Runtime > Change Runtime Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEMADUHesGjo"
   },
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "model_input = model_input.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcLpVrBdhEeP"
   },
   "source": [
    "### DEVICE performance without BT sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMlwVTzfhOs-"
   },
   "outputs": [],
   "source": [
    "model.encoder.transformer.layers.enable_nested_tensor=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7hMbLFbWwPH",
    "outputId": "fdacccb8-a60e-47ce-9d6b-caeaf61ad8c0"
   },
   "outputs": [],
   "source": [
    "print(\"slow path:\")\n",
    "print(\"==========\")\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "  for i in range(ITERATIONS):\n",
    "    output = model(model_input)\n",
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))\n",
    "\n",
    "print(\"fast path:\")\n",
    "print(\"==========\")\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "  with torch.no_grad():\n",
    "    for i in range(ITERATIONS):\n",
    "      output = model(model_input)\n",
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XC23NVChSYG"
   },
   "source": [
    "### DEVICE performance performance with BT sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4E54ia6YYkc"
   },
   "outputs": [],
   "source": [
    "model.encoder.transformer.layers.enable_nested_tensor = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TKPGx-3zaGXn",
    "outputId": "8ee5be47-dbab-4509-b23e-e0159fa77426"
   },
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model_input = model_input.to(DEVICE)\n",
    "\n",
    "print(\"slow path:\")\n",
    "print(\"==========\")\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "  for i in range(ITERATIONS):\n",
    "    output = model(model_input)\n",
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))\n",
    "\n",
    "print(\"fast path:\")\n",
    "print(\"==========\")\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "  with torch.no_grad():\n",
    "    for i in range(ITERATIONS):\n",
    "      output = model(model_input)\n",
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
